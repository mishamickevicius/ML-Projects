{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243fc6c3-d7e2-4e0c-b1c4-8962589b958b",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks and How to solve common problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cc4a1-8903-4acb-a81a-ee06dfb5e354",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a01fe-72a2-45f6-bbe8-27636e4f8cc9",
   "metadata": {},
   "source": [
    "During backpropagation, when going backwords and calculating the gradients to update the parameters, these gradients get smaller and smaller as they go to the lower levels. This can cause almost 0 changes in the parameters which means the model doesn't converge. This is called <b>The Vanishing Gradient Problem</b>. Also, the oppisite can happen, the gradients can start to become bigger and bigger which is called <b>The Exploding Gradient Problem</b>. Most deep neural networks suffer from unstable gradients, but there are a few ways to solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60fa6f-cd52-4686-a30b-385cbff62c40",
   "metadata": {},
   "source": [
    "### Glorot and He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55375e6e-6599-4de9-9f22-d5f577b7bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 12:13:46.682817: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-04 12:13:46.691750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736014426.702522    4460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736014426.705816    4460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-04 12:13:46.716690: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Keras uses Glorot Uniform initialization by default ###\n",
    "\n",
    "### He Init\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer='he_normal')\n",
    "\n",
    "### OR ###\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer=tf.keras.initializers.HeNormal())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898bb8f5-1205-4482-b114-ef324f695164",
   "metadata": {},
   "outputs": [],
   "source": [
    "### He init with uniform distribution based on fan avg\n",
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f22e-57c9-49b3-82bf-71312d556b47",
   "metadata": {},
   "source": [
    "### Better Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dca4f-fe69-46bb-9c7a-6f018a72e128",
   "metadata": {},
   "source": [
    "One of the reasons that Unstable Gradients happen is due to a poor choice in activation functions. <br>\n",
    "ReLU is a good activation function because it is quick to compute and does not saturate for positive values. However ReLU can cause a problem called <b>dying ReLUs</b>, which means that some neurons \"die\" or only output 0. This is caused when the weights get tweaked in a way that causes all inputs to neuron to be negative, and ReLU outputs 0 for all negatives. In some cases of neural networks, half of the neurons are \"dead\" especially when using high learning rates. <br>\n",
    "To solve this you can use a variation of ReLU called <b>Leaky ReLU</b> <br>\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$ <br>\n",
    "\n",
    "This variation ensures that neurons never die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b026e94-daca-4355-b351-7a0915334536",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f143c-5e75-417b-a5b8-1dd08e103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use LeakyReLU as a layer in the network instead of an activation function. It makes no difference\n",
    "model = tf.keras.models.Sequential([\n",
    "    # [...]  # more layers\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
    "    tf.keras.layers.LeakyReLU(negative_slope=0.2),  # activation as a separate layer\n",
    "    # [...]  # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17650d6-c880-4855-91ff-861ea5d38468",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using PReLU instead\n",
    "prelu = tf.keras.layers.PReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe7761-bd45-4b21-8c11-ee1175f712fa",
   "metadata": {},
   "source": [
    "A problem with ReLU, LeakyReLU, and PReLU, is that they are not smooth functions, meaning their derivative changes abruptly at x=0. This can cause the gradient descent to bounce around the optimum which makes it hard to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e04499-7418-44ad-897a-f8da03fcd296",
   "metadata": {},
   "source": [
    "#### ELU and SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4666c1-040f-48f4-ac05-07b88fc5e461",
   "metadata": {},
   "source": [
    "<b>Exponential Linear Unit (ELU)</b> is another activation function that can outperform ReLU and its variations in some cases. <br>\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$<br>\n",
    "Its advantages include: Taking on negative values when x < 0 which allows the unit to have an output closer to 0 which helps alleviate the vanishing gradients problem. Non-zero gradient at x=0 which avoids dead neurons, and when the function at $ \\alpha = 1 $ the function is smooth everywhere which means gradient descent converges faster. <br>\n",
    "<b>Note: Should always use He Initilzation with ELU, and ELU is slower than ReLU</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6468f69-79cf-41d8-8a7b-cfa0232a950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(50, activation='elu', \n",
    "                             kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b22a8f-9724-4d08-acf4-834b11e4cdfc",
   "metadata": {},
   "source": [
    "<b>Scaled ELU (SELU)</b> is another activation function that is a scaled variant of ELU. If you have a deep neural network where the hidden layers are just stacks of Dense layers using SELU, then the network will self-normalize: the output of each layer will tend to perserve a mean of 0 and a standard deviation of 1 during training which solves the vanishing gradient problem. This can cause SELU to outperform other activation functions. <br>\n",
    "Considerations to keep in mind about SELU:\n",
    "- The input features must be standardized: mean 0 and standard deviation 1.\n",
    "- Every hidden layer's weights must be initialized using LeCun normal init.\n",
    "- The Self-normalizing property is only guaranteed with plain MLPs. If you try SELU with other architectures, like recurrent neural networks or networks with skip connections(ex. Wide & Deep nets), it will not outperform ELU.\n",
    "- You cannot use regularization techniques with SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14145645-033d-40bd-b5a1-c1f10d5c6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(50, activation=\"selu\",\n",
    "                              kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9cb82-4d08-4332-89eb-9a726b89db72",
   "metadata": {},
   "source": [
    "SELU is not extremely popular or widly used due to the main considerations and is often outperformed by other activation functions like:<br>\n",
    "#### GELU, Swish, and Mish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d6d2e-890c-4688-84f5-b105ac2d4e3d",
   "metadata": {},
   "source": [
    "<b>GELU</b> is a smooth variant of the ReLU activation function. Due to its curvy/complex shape, gradient descent seems to find it easier to fit on to it. However, it is more computationaly expensive then the other activation functions and the performance boost doesn't always justify the extra cost. <br>\n",
    "<b> Sigmoid linear unit (SiLU) aka Swish</b> is very close to GELU but has one extra hyperparameter that can cause it to be more effective in certain cases <br>\n",
    "<b>Mish</b> is a smooth, nonconvex, and nonmonotonic variant of ReLU. It is similar or Swish and GELU.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c68694-2ff3-4a7a-bac9-f038b84be132",
   "metadata": {},
   "source": [
    "<b style=\"color: blue; font-size: 1.2em;\">Which Activation function should you use??</b><br>\n",
    "\n",
    "- ReLU is a good default for simple tasks: it's often just as good as the more sophisticated activation functions, plus its fast to compute and many libraries and hardware accelerators provide ReLU-specific optimizations.\n",
    "- Swish is probably a better default for more complex tasks.\n",
    "- Mish can give you slightly better results than Swish but at the cost of more computation time.\n",
    "- If you care about runtime latency, then LeakyReLU or parametrized leaky ReLU might be a better option.\n",
    "- For Deep MLPs, give SELU a try, but make sure to respect the constraints of it.\n",
    "- If you have time and computing power, try cross validation to find the best activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f668583-59d3-48b7-88eb-d3574f347c3e",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020d197-3f52-4879-b202-6a81fc735ad2",
   "metadata": {},
   "source": [
    "Batch normalization is another great technique to solving the vanishing/exploding gradent problem.<br> Batch Normalization works by normalizing/standardzing the data after the activation function. It scales the data just like StandardScaler or Normalization layer. It does this by evaluating the mean and standard eviation of the input over the current mini-batch. <br>\n",
    "Batch normalization can signifcantly reduce the vanishing gradient problem and even to the point where you can use old activation functions like tanh and maybe the sigmoid function. You can also use larger learning rates which speeds up the training process.<br> \n",
    "A problem caused by Batch Normalization is that it adds complexity to the model, meaning that each iteration will take longer due to more compuation required. However, this is usually worth it because BN can reduce the amount of epochs needed to reach the same performance.<br>\n",
    "A possible solution is to fuse the BN layer with the layer before it. This is done in libraries like TFLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df7bcb4-f0c1-4096-b025-712e738c6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/Desktop/env/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1736014427.863999    4460 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9530 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation='relu',\n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488baa0d-2556-4459-a3ea-957c19a05425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │         \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83665b77-caff-4fcf-a03b-39860625ff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gamma', True),\n",
       " ('beta', True),\n",
       " ('moving_mean', False),\n",
       " ('moving_variance', False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b40268-e153-4c13-8919-ad81ef351424",
   "metadata": {},
   "source": [
    "The main hyperparameters for Batch Normalization include:\n",
    "- momentum: a good momentum is around typically very close to 1.\n",
    "- axis: It determines which axis should be normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40572e8-a302-4ba8-a912-698cf1503c2a",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef3f6f-aaf0-401f-9c66-da2afd8780aa",
   "metadata": {},
   "source": [
    "Gradient clipping is when you clip the gradients during backpropagation so they never exceed some threshold. This helps mitigate the exploding gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c371540c-cae3-467b-9fcf-6aac3e96d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just set the clipvalue or clipnorm of the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25503f-e753-45eb-a4f6-1bd108b858d0",
   "metadata": {},
   "source": [
    "This will clip each component in the gradient vectors to a value between -1 and 1. There may be a problem with when you clip one of the compnents much more than the others. However, this can be fixed by using the clipnorm parameter instead of the clipvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f8c28-c791-4ea0-846b-dbb31ae4c6d7",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b306a-779f-4457-bab2-f2c37b558877",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch without first trying to find an existing neural network that accomplishes a similar task to the one you are trying to accomplish. <b>(How to find them is disscused in Chapter 14)</b> If you find such a model then you can generally reuse most of its layers, except for the top ones. This technique is called <b><i>Transfer Learning</i></b>. It will not only speed up training but require much less training data.<br><br>\n",
    "Note: If the input shape of your task doesn't match the input shape of the model whose layers you are reusing, you will need to add some type of preprocessing layer to reshape the input. <br>\n",
    "If the tasks are very similar you should first try to reuse all the layers except for the output. You can lock the weights of the layers you are reusing so you only train the layers you are adding. Start here and change the number of reused layers until you find an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f350ff-66e0-4fbd-ae71-1b2148669783",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28298dc8-44d5-431d-961a-552d4ace8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11117273-9508-4b6d-ac34-cc8923e2ce83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736014429.130888    4526 service.cc:148] XLA service 0x781458004970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736014429.130903    4526 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-01-04 12:13:49.139404: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1736014429.172242    4526 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-04 12:13:49.187318: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 203/1376\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.2718 - loss: 1.9251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736014431.137497    4526 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5491 - loss: 1.4140 - val_accuracy: 0.7904 - val_loss: 0.6669\n",
      "Epoch 2/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877us/step - accuracy: 0.8082 - loss: 0.6222 - val_accuracy: 0.8383 - val_loss: 0.5043\n",
      "Epoch 3/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8505 - loss: 0.4878 - val_accuracy: 0.8574 - val_loss: 0.4306\n",
      "Epoch 4/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - accuracy: 0.8669 - loss: 0.4195 - val_accuracy: 0.8709 - val_loss: 0.3878\n",
      "Epoch 5/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step - accuracy: 0.8764 - loss: 0.3780 - val_accuracy: 0.8787 - val_loss: 0.3605\n",
      "Epoch 6/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step - accuracy: 0.8832 - loss: 0.3505 - val_accuracy: 0.8852 - val_loss: 0.3415\n",
      "Epoch 7/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903us/step - accuracy: 0.8880 - loss: 0.3306 - val_accuracy: 0.8882 - val_loss: 0.3276\n",
      "Epoch 8/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883us/step - accuracy: 0.8926 - loss: 0.3156 - val_accuracy: 0.8907 - val_loss: 0.3172\n",
      "Epoch 9/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 891us/step - accuracy: 0.8968 - loss: 0.3038 - val_accuracy: 0.8920 - val_loss: 0.3087\n",
      "Epoch 10/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 900us/step - accuracy: 0.8994 - loss: 0.2940 - val_accuracy: 0.8970 - val_loss: 0.3017\n",
      "Epoch 11/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step - accuracy: 0.9023 - loss: 0.2859 - val_accuracy: 0.8985 - val_loss: 0.2959\n",
      "Epoch 12/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step - accuracy: 0.9047 - loss: 0.2790 - val_accuracy: 0.8997 - val_loss: 0.2910\n",
      "Epoch 13/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866us/step - accuracy: 0.9064 - loss: 0.2730 - val_accuracy: 0.9015 - val_loss: 0.2864\n",
      "Epoch 14/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step - accuracy: 0.9080 - loss: 0.2676 - val_accuracy: 0.9035 - val_loss: 0.2825\n",
      "Epoch 15/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step - accuracy: 0.9095 - loss: 0.2628 - val_accuracy: 0.9042 - val_loss: 0.2790\n",
      "Epoch 16/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853us/step - accuracy: 0.9111 - loss: 0.2585 - val_accuracy: 0.9047 - val_loss: 0.2758\n",
      "Epoch 17/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - accuracy: 0.9121 - loss: 0.2544 - val_accuracy: 0.9055 - val_loss: 0.2730\n",
      "Epoch 18/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - accuracy: 0.9134 - loss: 0.2508 - val_accuracy: 0.9065 - val_loss: 0.2702\n",
      "Epoch 19/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step - accuracy: 0.9146 - loss: 0.2473 - val_accuracy: 0.9065 - val_loss: 0.2678\n",
      "Epoch 20/20\n",
      "\u001b[1m1376/1376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step - accuracy: 0.9155 - loss: 0.2441 - val_accuracy: 0.9070 - val_loss: 0.2654\n"
     ]
    }
   ],
   "source": [
    "# extra code – split Fashion MNIST into tasks A and B, then train and save\n",
    "#              model A to \"my_model_A\".\n",
    "import numpy as np\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aefcf1b6-ebcf-4453-b59a-5cc6a14d80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assuming model A was trained and saved\n",
    "model_A = tf.keras.models.load_model('my_model_A.keras')\n",
    "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])  ## Reuse all layers except for output\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation='sigmoid')) ## Add the output layer that matches our task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd5e28-e8a2-4595-9fc4-2a28cbab48a2",
   "metadata": {},
   "source": [
    "<b>Note:</b> <br>\n",
    "Since Model B uses the same layers as Model A, when you train Model B you will also affect Model A. To fix this make a copy and set the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0aa504e-0e73-4107-a530-80998556941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "model_A_clone = tf.keras.models.clone_model(model_A)  #### Doesn't clone the weights only the architecture ####\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341432f-10f6-446f-a209-8d5762f4c979",
   "metadata": {},
   "source": [
    "To prevent the reused weights from being changed dramaticly due to large loss at the beginning, freeze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fef3aca7-9579-4284-919c-d16f01f5e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "### Must always compile after freezing or unfreezing layers\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5316ac2f-0c8f-4fb1-ba50-6d826ab98293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.2765 - loss: 1.1583 - val_accuracy: 0.2819 - val_loss: 1.0263\n",
      "Epoch 2/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1496 - loss: 1.0851 - val_accuracy: 0.2473 - val_loss: 0.9920\n",
      "Epoch 3/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1566 - loss: 1.0430 - val_accuracy: 0.2562 - val_loss: 0.9637\n",
      "Epoch 4/4\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1605 - loss: 1.0093 - val_accuracy: 0.2671 - val_loss: 0.9375\n",
      "Epoch 1/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - accuracy: 0.1806 - loss: 0.9578 - val_accuracy: 0.3996 - val_loss: 0.8364\n",
      "Epoch 2/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3223 - loss: 0.8486 - val_accuracy: 0.4748 - val_loss: 0.7529\n",
      "Epoch 3/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4613 - loss: 0.7575 - val_accuracy: 0.5282 - val_loss: 0.6868\n",
      "Epoch 4/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5584 - loss: 0.6859 - val_accuracy: 0.6212 - val_loss: 0.6353\n",
      "Epoch 5/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6784 - loss: 0.6299 - val_accuracy: 0.7043 - val_loss: 0.5927\n",
      "Epoch 6/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7465 - loss: 0.5833 - val_accuracy: 0.7626 - val_loss: 0.5571\n",
      "Epoch 7/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8072 - loss: 0.5448 - val_accuracy: 0.8081 - val_loss: 0.5266\n",
      "Epoch 8/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8273 - loss: 0.5120 - val_accuracy: 0.8427 - val_loss: 0.4997\n",
      "Epoch 9/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8652 - loss: 0.4831 - val_accuracy: 0.8675 - val_loss: 0.4758\n",
      "Epoch 10/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8987 - loss: 0.4575 - val_accuracy: 0.8892 - val_loss: 0.4543\n",
      "Epoch 11/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9083 - loss: 0.4348 - val_accuracy: 0.8981 - val_loss: 0.4351\n",
      "Epoch 12/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9253 - loss: 0.4143 - val_accuracy: 0.9041 - val_loss: 0.4176\n",
      "Epoch 13/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9322 - loss: 0.3957 - val_accuracy: 0.9060 - val_loss: 0.4019\n",
      "Epoch 14/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9322 - loss: 0.3789 - val_accuracy: 0.9060 - val_loss: 0.3876\n",
      "Epoch 15/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9322 - loss: 0.3638 - val_accuracy: 0.9100 - val_loss: 0.3746\n",
      "Epoch 16/16\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9372 - loss: 0.3503 - val_accuracy: 0.9149 - val_loss: 0.3628\n"
     ]
    }
   ],
   "source": [
    "### You can train the model for a few epochs then unfreeze the weights to really optimize the new model\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# Unfreeze layers\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "### Must always compile after freezing or unfreezing layers\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72379a5-1a7f-4949-af88-cf3b4bcafa76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Transfer learning does not typically work well with small dense networks because small dense networks learn few patterns. Dense networks typically learn very specific patterns which are unlikly to be useful in other tasks.\n",
    "</div> \n",
    "<b>Transfer Learning works best with deep convolutional neural networks with tend to learn feature detectors that are more general</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f2d16-8b20-4138-8171-c74196807e35",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e736b-8573-4b32-a304-05063e4756e2",
   "metadata": {},
   "source": [
    "Suppose you want to tackle a complex task for which you don't have much labeled training data, but you also can't find a model that was trained on a similar task. <br>\n",
    "First you should always try and find more labeled data. However, if you can easily get unlabeled data, then you can try <b>Unsupervised Pretraining</b> This envolves training an autoencoder or GAN (See Chapter 17) on this unlabeled data and reuse its lower level layers in your DNN, then train the DNN with the small labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d278-09bd-474e-941c-2517aeb1b97a",
   "metadata": {},
   "source": [
    "### Pretraing on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f3854-c37d-44b7-a92c-d7a62853bfe8",
   "metadata": {},
   "source": [
    "If you do not have much labeled data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual tasks. \n",
    "<br> \n",
    "One example in computer vision is the following: Say you want to build a system to recognize faces, but you only have a few pictures of each individual. Instead of trying to gather more pictures of the specific individuals, you can get lots of pictures of random people from the internet and train a first neural network to try and predict if the people in the images are the same person. The lower layers of this  neural network would be able to learn the lower level features of a face which can be reused in your actually tasks.\n",
    "<br> \n",
    "Another example in NLP applications, you can download a corpus of millions of text documents and automatically generate labeled data from it. You can mask out words and try and make a neural network to predict the missing word. Then you can reuse the lower level layers of that network for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674af54-9606-4681-ac4c-f1792eeda271",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca582a-e2c5-4aea-b732-96173a616acf",
   "metadata": {},
   "source": [
    "Training Deep NNs is a slow process. So far there are four ways to speed up training:\n",
    "- Applying a good initialization strategy for the connection weights\n",
    "- Using a good activation function\n",
    "- Using Batch Normalization\n",
    "- Reuseing parts of a pretrained network(Possibly built from an auxiliary task or unsupervised learning task)\n",
    "\n",
    "<b> Another way</b> is to use a faster optimizer rather than regular gradient descent optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d99ca-3c6d-4814-a68b-55527adc524d",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c61b1-ed89-44d5-97c4-fe6fd9c04036",
   "metadata": {},
   "source": [
    "Regular gradient descent goes step by step, updating the weigths based on the calculated gradient and the step size.<br> <b>Momentum optimization</b> uses previous calculated gradients as an acceleration, not as a speed. It calculates a momentum vector and then uses it along with the gradient to determine how to change the weights, but it keeps in mind the previous gradients to increases its speed or to roll down the hill faster.<br>\n",
    "To keep into account \"friction\" of the hill, it has a <b><i>momentum</i></b> hyperparameter that is a value between 0 (high friction) and 1 (no friction). The typical value for this parameter is 0.9<br>\n",
    "<br>\n",
    "Momentum optimization helps speed up the time to find the minimum and can better escape local minimums. However, it can bounce back and forth around the minimum which is why having a little bit of friction is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f61ff53a-a414-48bc-a2a5-02a09eb01b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the SGD with the momentum hyperparameter\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6f128-4040-4ab6-a05c-c32bdacae1e9",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5754853-4e63-4030-b020-d48a83c9b33b",
   "metadata": {},
   "source": [
    "NAG is a variation of Momentum that calculates the gradient ahead which allows NAG to converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3974bc1c-e976-4322-b919-01d6289fc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAG implementation\n",
    "opimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
    "                                  nesterov=True) ## Extra parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60023bde-d5d6-4c33-9ba3-b47da2691aac",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35d130-5e1d-4a74-bb34-de4114f1cbde",
   "metadata": {},
   "source": [
    "AdaGrad is where the learning rate is decayed depending on how steep the slope is. This is called adaptive learning rate. However AdaGrad has a risk of slowing down too fast and never converging, which is where <b>RMSProp</b> comes in. It fixes this by accumulating only the gradients from the most recent iterations, as opposed to all the gradients since the beginning of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a2cb4aa-141b-4546-8f68-adfa6a8bd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparamater rho is the decay rate. \n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded2f1e-c624-47a8-a130-792b8b7f7bc7",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e6c79-d22b-403a-bcde-aa1df9179399",
   "metadata": {},
   "source": [
    "The <b>Adam</b> optimizer combines the ideas of Momentum optimizing and RMSProp. It keeps track of an exponentially decaying average of past gradients like Momentum and keeps track of an exponentially decaying average of past squared gradients, like RMSProp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4884169-8882-4bac-8adf-a5418b234139",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7f109-2336-4684-b740-0be8ce78adea",
   "metadata": {},
   "source": [
    "### Adam Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e57ca-62ac-4d07-9eb2-424f6e477043",
   "metadata": {},
   "source": [
    "- AdaMax: This variation can be unstable and doesn't always preform better than Adam\n",
    "- Nadam: This is a variation of Adam that uses the Nesterov trick. It can often converge faster and outperform Adam but can also be outperformed by RMSProp.\n",
    "- AdamW: This is a variation of Adam that adds a regularization technique called weight decay. This can keep the weights small which can boost performance.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<b>Adaptive optimization methods like RMSProp, Adam, and its variations are often great at converging fast to a good solution. However, they can lead to solutions that generalize poorly on some datasets. So if the results are not good, use Nesterov Accelerated Gradient.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c62e2b-6be7-4fa4-b5ed-1b3061c08af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of Adam Variations\n",
    "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, beta_1=0.9, beta_2=0.999, weight_decay=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606d754-79e6-4bcb-b7b8-fe0f1aeecaac",
   "metadata": {},
   "source": [
    "## Sparse Models\n",
    "<br>\n",
    "If you need a very fast model at runtime or if you have low memory amounts, using a sparse model might be a good idea. A way to get these models is to apply strong l1 regularization during training, similar to Lasso Regression.\n",
    "<br><br>\n",
    "<b>The Tensorflow Model Optimization Toolkit provides an API to make these models easily.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1e318-2da7-4c46-8f34-20492035bdab",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c11445b-abdf-48d0-b8ac-1bb9b7298fee",
   "metadata": {},
   "source": [
    "Learning Rate Scheduling is when you change the learning rate during the training process based on some results<br>\n",
    "Different common LRS methods:\n",
    "- Power Scheduling\n",
    "- Exponential Scheduling\n",
    "- Piecewise constant scheduling\n",
    "- Performance scheduling\n",
    "- 1cycle scheduling\n",
    "- Etc\n",
    "<br><b>All of these can be good options</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b149ebc-82a8-4e89-b77d-481ce8b7682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exponential Scheduling implmentation\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37864d8e-bf16-4a1e-819c-b242c4f4d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This callback will update the learning_rate attribute at the beginning of each epoch\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train, y_train, epochs=5, callbacks=[lr_scheduler])\n",
    "## history.history['lr'] should give you the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c42df06d-96be-4e1e-877e-0ec65b771a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The schedule function can also take the current learning rate as a parameter\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89050215-a700-457f-9717-48bc032430f5",
   "metadata": {},
   "source": [
    "<b>When you save a model, the optimizer and its learning rate get saved with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off. This doesn't work if your scheduler uses the epoch parameter only, because epoch count doesn't get saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da60d0f7-d2c0-4c05-b64a-603de2d3ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf27f96-6693-450a-b1ae-ad2397a0dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the ReduceLROnPlateau callback for performance scheduling.\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903a224-a506-4d3e-9239-fa9657cf04e1",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ca853-7548-4226-956c-7f1669954c3d",
   "metadata": {},
   "source": [
    "Due to deep neural networks many parameters they can learn complex patterns. However, this makes them prone to overfitting. <b>Good Regularization techniques, like EarlyStopping, are needed to prevent this</b><br>\n",
    "Here are some other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2152244-ac78-4de5-8d34-fee86f68dafd",
   "metadata": {},
   "source": [
    "### l1 and l2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718a4433-f51b-45a1-9004-419f77c2bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To use l1 and l2 just like in simple linear models, you can use them in deep neural networks\n",
    "layer = tf.keras.layers.Dense(100, activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(0.01)) # Same for l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5784567-77f0-4f33-8f07-feb555c0c02d",
   "metadata": {},
   "source": [
    "### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56aed8-970b-4acd-a7e5-983fb0fce6f4",
   "metadata": {},
   "source": [
    "Dropout layers work by temporarily disabling a percentage of neurons during each training step. The percentage is controlled by the dropout rate which is typically a value between 10% and 50%. After training neurons don't get dropped anymore. This forces the neurons that don't get dropped to \"learn better\". This technique has been proven to be very affective. Dropout tends to lead to slower convergence but leads to better models especially for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957df830-74ff-4f64-9c28-dbe2862abe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=[28, 28]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972790a-444e-4a0b-9606-6e9235f68e59",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b1211e-7381-45e0-9894-a678c066c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_probs = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "\n",
    "y_proba = y_probs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a52f4243-6eef-4592-b201-266df007fca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07, 0.08, 0.1 , 0.13, 0.09, 0.13, 0.08, 0.12, 0.11, 0.08]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:1]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11c5f712-8616-46ef-9976-26522544e598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07, 0.07, 0.1 , 0.13, 0.07, 0.16, 0.08, 0.14, 0.1 , 0.08],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34d53b72-7f71-461f-96b4-72732f7d17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa599da-775f-4c30-a0dc-172ecac034b6",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4f8a389-d098-427c-9d45-bc90154104d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    100, activation='relu', kernel_initializer='he_normal',\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd21f3e-459b-4b04-ad99-203442300699",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ccaa5-c242-4f9d-8474-a73f5a4d53a5",
   "metadata": {},
   "source": [
    "## Default DNN Configuration\n",
    "| Hyperparameter       | Default Value         |\n",
    "|----------------------|-----------------------|\n",
    "| Kernel Initializer | He Initialization |\n",
    "| Activation Function | ReLU if shallow; Swish if deep |\n",
    "| Normalization | None if shallow; batch norm if deep |\n",
    "| Regularization | Early stopping; weight decay if needed |\n",
    "| Optimizer | Nesterov accelerated gradients or AdamW |\n",
    "| Learning rate schedule | Performance scheduling or 1cycle |\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Not Hard Rules. Still try other methods and think about which ones fit best</b>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1665cb-2965-44ab-928a-ef311da6f93b",
   "metadata": {},
   "source": [
    "## DNN configuration for a self-normalizing net\n",
    "| Hyperparameter       | Default Value         |\n",
    "|----------------------|-----------------------|\n",
    "| Kernel Initializer | LeCun initialization |\n",
    "| Activation Function | SELU |\n",
    "| Normalization | None(Self-Normalizaing) |\n",
    "| Regularization | Alpha Dropout if needed |\n",
    "| Optimizer | Nesterov accelerated gradients |\n",
    "| Learning rate schedule | Performance scheduling or 1cycle |\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Not Hard Rules. Still try other methods and think about which ones fit best</b>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7c0fd-75af-450a-ac99-ef740a317415",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7ed24-394b-4538-9242-81403a3bc72d",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f2d59f1-4353-4667-94f9-72d2216cfe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0edca97e-0c8a-4527-80a2-923a70b24e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1007 - loss: nan - val_accuracy: 0.0996 - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78156c5ab770>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_val, y_val = X_train[:5000], y_train[:5000]\n",
    "X_train, y_train = X_train[5000:], y_train[5000:]\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[32,32,3]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "## 20 Hidden layers \n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, activation='swish', \n",
    "    kernel_initializer='he_normal'))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-6)\n",
    "model.compile(optimizer=opimizer,\n",
    "         loss='sparse_categorical_crossentropy',\n",
    "         metrics=['accuracy'])\n",
    "import datetime\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=20, \n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "log_dir = \"logs/my_cifar10_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping, tensorboard_callback],\n",
    "     validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75658992-0dd6-48ab-943e-5829e9d2d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901618a1-0de4-4237-ac1d-13cded2822e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
