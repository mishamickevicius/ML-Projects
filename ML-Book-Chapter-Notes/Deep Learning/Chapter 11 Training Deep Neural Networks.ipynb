{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243fc6c3-d7e2-4e0c-b1c4-8962589b958b",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks and How to solve common problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cc4a1-8903-4acb-a81a-ee06dfb5e354",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a01fe-72a2-45f6-bbe8-27636e4f8cc9",
   "metadata": {},
   "source": [
    "During backpropagation, when going backwords and calculating the gradients to update the parameters, these gradients get smaller and smaller as they go to the lower levels. This can cause almost 0 changes in the parameters which means the model doesn't converge. This is called <b>The Vanishing Gradient Problem</b>. Also, the oppisite can happen, the gradients can start to become bigger and bigger which is called <b>The Exploding Gradient Problem</b>. Most deep neural networks suffer from unstable gradients, but there are a few ways to solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60fa6f-cd52-4686-a30b-385cbff62c40",
   "metadata": {},
   "source": [
    "### Glorot and He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55375e6e-6599-4de9-9f22-d5f577b7bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 19:04:04.954262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-29 19:04:05.093603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735520645.147522    3975 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735520645.167252    3975 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-29 19:04:05.345079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "### Keras uses Glorot Uniform initialization by default ###\n",
    "\n",
    "### He Init\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer='he_normal')\n",
    "\n",
    "### OR ###\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer=tf.keras.initializers.HeNormal())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898bb8f5-1205-4482-b114-ef324f695164",
   "metadata": {},
   "outputs": [],
   "source": [
    "### He init with uniform distribution based on fan avg\n",
    "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "dense = tf.keras.layers.Dense(50, activation='relu', \n",
    "                             kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3f22e-57c9-49b3-82bf-71312d556b47",
   "metadata": {},
   "source": [
    "### Better Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dca4f-fe69-46bb-9c7a-6f018a72e128",
   "metadata": {},
   "source": [
    "One of the reasons that Unstable Gradients happen is due to a poor choice in activation functions. <br>\n",
    "ReLU is a good activation function because it is quick to compute and does not saturate for positive values. However ReLU can cause a problem called <b>dying ReLUs</b>, which means that some neurons \"die\" or only output 0. This is caused when the weights get tweaked in a way that causes all inputs to neuron to be negative, and ReLU outputs 0 for all negatives. In some cases of neural networks, half of the neurons are \"dead\" especially when using high learning rates. <br>\n",
    "To solve this you can use a variation of ReLU called <b>Leaky ReLU</b> <br>\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$ <br>\n",
    "\n",
    "This variation ensures that neurons never die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b026e94-daca-4355-b351-7a0915334536",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = tf.keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f143c-5e75-417b-a5b8-1dd08e103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use LeakyReLU as a layer in the network instead of an activation function. It makes no difference\n",
    "model = tf.keras.models.Sequential([\n",
    "    # [...]  # more layers\n",
    "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
    "    tf.keras.layers.LeakyReLU(negative_slope=0.2),  # activation as a separate layer\n",
    "    # [...]  # more layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17650d6-c880-4855-91ff-861ea5d38468",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using PReLU instead\n",
    "prelu = tf.keras.layers.PReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe7761-bd45-4b21-8c11-ee1175f712fa",
   "metadata": {},
   "source": [
    "A problem with ReLU, LeakyReLU, and PReLU, is that they are not smooth functions, meaning their derivative changes abruptly at x=0. This can cause the gradient descent to bounce around the optimum which makes it hard to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e04499-7418-44ad-897a-f8da03fcd296",
   "metadata": {},
   "source": [
    "#### ELU and SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4666c1-040f-48f4-ac05-07b88fc5e461",
   "metadata": {},
   "source": [
    "<b>Exponential Linear Unit (ELU)</b> is another activation function that can outperform ReLU and its variations in some cases. <br>\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$<br>\n",
    "Its advantages include: Taking on negative values when x < 0 which allows the unit to have an output closer to 0 which helps alleviate the vanishing gradients problem. Non-zero gradient at x=0 which avoids dead neurons, and when the function at $ \\alpha = 1 $ the function is smooth everywhere which means gradient descent converges faster. <br>\n",
    "<b>Note: Should always use He Initilzation with ELU, and ELU is slower than ReLU</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6468f69-79cf-41d8-8a7b-cfa0232a950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(50, activation='elu', \n",
    "                             kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b22a8f-9724-4d08-acf4-834b11e4cdfc",
   "metadata": {},
   "source": [
    "<b>Scaled ELU (SELU)</b> is another activation function that is a scaled variant of ELU. If you have a deep neural network where the hidden layers are just stacks of Dense layers using SELU, then the network will self-normalize: the output of each layer will tend to perserve a mean of 0 and a standard deviation of 1 during training which solves the vanishing gradient problem. This can cause SELU to outperform other activation functions. <br>\n",
    "Considerations to keep in mind about SELU:\n",
    "- The input features must be standardized: mean 0 and standard deviation 1.\n",
    "- Every hidden layer's weights must be initialized using LeCun normal init.\n",
    "- The Self-normalizing property is only guaranteed with plain MLPs. If you try SELU with other architectures, like recurrent neural networks or networks with skip connections(ex. Wide & Deep nets), it will not outperform ELU.\n",
    "- You cannot use regularization techniques with SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14145645-033d-40bd-b5a1-c1f10d5c6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(50, activation=\"selu\",\n",
    "                              kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9cb82-4d08-4332-89eb-9a726b89db72",
   "metadata": {},
   "source": [
    "SELU is not extremely popular or widly used due to the main considerations and is often outperformed by other activation functions like:<br>\n",
    "#### GELU, Swish, and Mish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d6d2e-890c-4688-84f5-b105ac2d4e3d",
   "metadata": {},
   "source": [
    "<b>GELU</b> is a smooth variant of the ReLU activation function. Due to its curvy/complex shape, gradient descent seems to find it easier to fit on to it. However, it is more computationaly expensive then the other activation functions and the performance boost doesn't always justify the extra cost. <br>\n",
    "<b> Sigmoid linear unit (SiLU) aka Swish</b> is very close to GELU but has one extra hyperparameter that can cause it to be more effective in certain cases <br>\n",
    "<b>Mish</b> is a smooth, nonconvex, and nonmonotonic variant of ReLU. It is similar or Swish and GELU.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c68694-2ff3-4a7a-bac9-f038b84be132",
   "metadata": {},
   "source": [
    "<b style=\"color: blue; font-size: 1.2em;\">Which Activation function should you use??</b><br>\n",
    "\n",
    "- ReLU is a good default for simple tasks: it's often just as good as the more sophisticated activation functions, plus its fast to compute and many libraries and hardware accelerators provide ReLU-specific optimizations.\n",
    "- Swish is probably a better default for more complex tasks.\n",
    "- Mish can give you slightly better results than Swish but at the cost of more computation time.\n",
    "- If you care about runtime latency, then LeakyReLU or parametrized leaky ReLU might be a better option.\n",
    "- For Deep MLPs, give SELU a try, but make sure to respect the constraints of it.\n",
    "- If you have time and computing power, try cross validation to find the best activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f668583-59d3-48b7-88eb-d3574f347c3e",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020d197-3f52-4879-b202-6a81fc735ad2",
   "metadata": {},
   "source": [
    "Batch normalization is another great technique to solving the vanishing/exploding gradent problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7bcb4-f0c1-4096-b025-712e738c6bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
