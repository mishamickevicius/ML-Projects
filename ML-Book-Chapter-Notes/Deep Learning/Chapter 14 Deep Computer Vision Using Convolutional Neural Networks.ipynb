{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3856214-dc1a-4e96-bf75-4e25cecc58e9",
   "metadata": {},
   "source": [
    "# Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b81b1-e249-4bee-beb2-3cc229e831cd",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layers with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72a4b78-6f08-4a09-8610-fe1f6bf9847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 15:29:59.613574: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-12 15:29:59.792135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736717399.856608    3985 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736717399.876772    3985 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-12 15:30:00.055834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1736717401.995920    3985 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9697 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "## Get some sample images\n",
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()['images']\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f4a54e-425d-4621-a733-cf7ff3c0176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape\n",
    "## 4d tensor\n",
    "## 2 Images in tensor\n",
    "## 70 is the height in pixels\n",
    "## 120 is the width in pixels\n",
    "## 3 is the amount of color channels(most likly RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448b0578-160e-4003-bf1f-2b3934367cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736717402.623201    3985 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc65c7cd-6e0d-4a0c-b339-650c99bd4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape\n",
    "## 32 channels (because of the filters)\n",
    "## The height and width shrunk because Conv2d doesn't zero pad by defualt\n",
    "## 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82430eb9-4acf-42ff-a956-aef66856198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
    "                                   padding='same') ## Zero padding\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc668b6e-2582-4c66-8517-2f29ed39d095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf9030a-8405-4471-9806-514e0a92ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "biases.shape\n",
    "## 1d shape [output_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16871da-c0a6-4c30-a64a-21a15a350493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels.shape\n",
    "## 4d shape [kernel_height, kernel_width, input_channels, output_channels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61920a4-bd8b-43f3-a26d-cefffa6f92a4",
   "metadata": {},
   "source": [
    "<b>You should also add activation functions and kernel_initilizers to Convolutional layers, for the same reasons as Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f7a70-fdf6-4acd-a7a4-152c504a70f5",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981accab-297f-4d9e-b18a-a2c6ad3ab340",
   "metadata": {},
   "source": [
    "Pooling layers are ment to subsample(i.e. shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters(thereby limiting the risk of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ec1b-d983-48c9-a36b-fb696d335089",
   "metadata": {},
   "source": [
    "### Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b08449c-b7d5-4e0d-a563-34dff7557514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Pool\n",
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n",
    "## AveragePool\n",
    "avg_pool = tf.keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4159347-496b-40ba-bf31-8f669e83a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depthwise pooling layer\n",
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1] // self.pool_size ## number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5038703-25cc-42c6-881d-bcec53e6e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global average pooling\n",
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2636dbc3-2c66-43c3-bc42-08e33410e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.643388  , 0.59718215, 0.5825038 ],\n",
       "       [0.7630747 , 0.26010972, 0.10848834]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ff719-a704-4243-bf1a-64aa897d502e",
   "metadata": {},
   "source": [
    "## CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72237c1-121b-41df-8b6e-36430bfb8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads the mnist dataset, add the channels axis to the inputs,\n",
    "#              scales the values to the 0-1 range, and splits the dataset\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78204d-839e-4b56-9ea0-a4c526573554",
   "metadata": {},
   "source": [
    "### Basic CNN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d18178-2f3e-4861-bf66-d7575367caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/Desktop/env/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Used on Fashion MNIST\n",
    "from functools import partial\n",
    "\n",
    "DefaultConv2d = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                       activation='relu', kernel_initializer='he_normal')\n",
    "model = tf.keras.Sequential([\n",
    "    ### This first conv layer should capture lower level features so less filters\n",
    "    DefaultConv2d(filters=64, kernel_size=7, input_shape=[28,28,1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2d(filters=128),\n",
    "    DefaultConv2d(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    ### This conv layers should capture the high level features so have more filters\n",
    "    DefaultConv2d(filters=256),\n",
    "    DefaultConv2d(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b359727-f2be-40f5-815b-a7beaab0dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736718792.413770    4050 service.cc:148] XLA service 0x7f9e0c004d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736718792.413897    4050 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-01-12 15:53:12.482007: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  33/1719\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1556 - loss: 2.6460    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736718795.088315    4050 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6032 - loss: 1.1305 - val_accuracy: 0.8650 - val_loss: 0.3915\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8477 - loss: 0.4480 - val_accuracy: 0.8834 - val_loss: 0.3058\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8781 - loss: 0.3611 - val_accuracy: 0.8944 - val_loss: 0.3026\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8970 - loss: 0.3063 - val_accuracy: 0.9036 - val_loss: 0.2577\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9002 - loss: 0.2906 - val_accuracy: 0.9058 - val_loss: 0.2568\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9093 - loss: 0.2643 - val_accuracy: 0.9124 - val_loss: 0.2468\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9160 - loss: 0.2457 - val_accuracy: 0.9070 - val_loss: 0.2605\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9211 - loss: 0.2341 - val_accuracy: 0.9160 - val_loss: 0.2416\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.2300 - val_accuracy: 0.9032 - val_loss: 0.2677\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9263 - loss: 0.2101 - val_accuracy: 0.9112 - val_loss: 0.2700\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "             optimizer='nadam',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688effd-387a-45ca-b93c-4cb37980aeda",
   "metadata": {},
   "source": [
    "### List of other Architectures\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- GoogLeNet\n",
    "- VGGNet\n",
    "- ResNet\n",
    "- Xception(A variant of GoogLeNet)\n",
    "- SENet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "- MobileNet\n",
    "- CSPNet\n",
    "- EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06cb8-10ce-4432-84c9-26411aee49ac",
   "metadata": {},
   "source": [
    "## Implementing a ResNet-34 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f23db35-7c40-4b7b-ac23-5019d4e05e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                       padding='same', kernel_initializer='he_normal',\n",
    "                       use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs \n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2875f4d-fe47-4d5e-852a-0df7a00e9a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/Desktop/env/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "res_net_34 = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    res_net_34.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "    \n",
    "res_net_34.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "res_net_34.add(tf.keras.layers.Flatten())\n",
    "res_net_34.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfe862-ae42-4c8c-b7e2-11ba24e2e888",
   "metadata": {},
   "source": [
    "## Using Pretrained Models from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d08d6ab-a0af-4149-8b78-ae1883b66b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a074114b-4daf-4337-9cd1-0b4bea56d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()['images']\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "667b3b65-d128-4395-a5a4-7380c3f6a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24de1240-6050-4468-bdd4-1a05fad41cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(inputs)\n",
    "y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88552efa-6b98-4a55-b565-5b5b8dea8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n03598930 - jigsaw_puzzle 30.61%\n",
      "  n02782093 - balloon      17.17%\n",
      "  n03888257 - parachute    5.58%\n",
      "  n06359193 - web_site     3.83%\n",
      "Image #1\n",
      "  n04209133 - shower_cap   34.29%\n",
      "  n09229709 - bubble       11.40%\n",
      "  n02782093 - balloon      9.46%\n",
      "  n07745940 - strawberry   4.94%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(y_proba, top=4)\n",
    "for image_index in range(len(images)):\n",
    "    print(f'Image #{image_index}')\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f'  {class_id} - {name:12s} {y_proba:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03639f-7258-4904-bfae-8b485ba58087",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2270ca9-b21d-425c-bde5-38c983c8ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
