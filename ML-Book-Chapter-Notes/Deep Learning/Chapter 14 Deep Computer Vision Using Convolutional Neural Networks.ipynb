{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3856214-dc1a-4e96-bf75-4e25cecc58e9",
   "metadata": {},
   "source": [
    "# Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b81b1-e249-4bee-beb2-3cc229e831cd",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layers with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72a4b78-6f08-4a09-8610-fe1f6bf9847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 19:48:45.347131: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-12 19:48:45.485408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736732925.539547    3929 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736732925.555388    3929 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-12 19:48:45.697557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1736732927.708197    3929 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9587 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "## Get some sample images\n",
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()['images']\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f4a54e-425d-4621-a733-cf7ff3c0176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape\n",
    "## 4d tensor\n",
    "## 2 Images in tensor\n",
    "## 70 is the height in pixels\n",
    "## 120 is the width in pixels\n",
    "## 3 is the amount of color channels(most likly RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448b0578-160e-4003-bf1f-2b3934367cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736732928.335240    3929 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc65c7cd-6e0d-4a0c-b339-650c99bd4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape\n",
    "## 32 channels (because of the filters)\n",
    "## The height and width shrunk because Conv2d doesn't zero pad by defualt\n",
    "## 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82430eb9-4acf-42ff-a956-aef66856198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
    "                                   padding='same') ## Zero padding\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc668b6e-2582-4c66-8517-2f29ed39d095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf9030a-8405-4471-9806-514e0a92ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "biases.shape\n",
    "## 1d shape [output_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16871da-c0a6-4c30-a64a-21a15a350493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels.shape\n",
    "## 4d shape [kernel_height, kernel_width, input_channels, output_channels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61920a4-bd8b-43f3-a26d-cefffa6f92a4",
   "metadata": {},
   "source": [
    "<b>You should also add activation functions and kernel_initilizers to Convolutional layers, for the same reasons as Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f7a70-fdf6-4acd-a7a4-152c504a70f5",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981accab-297f-4d9e-b18a-a2c6ad3ab340",
   "metadata": {},
   "source": [
    "Pooling layers are ment to subsample(i.e. shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters(thereby limiting the risk of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ec1b-d983-48c9-a36b-fb696d335089",
   "metadata": {},
   "source": [
    "### Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b08449c-b7d5-4e0d-a563-34dff7557514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Pool\n",
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n",
    "## AveragePool\n",
    "avg_pool = tf.keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4159347-496b-40ba-bf31-8f669e83a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depthwise pooling layer\n",
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1] // self.pool_size ## number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5038703-25cc-42c6-881d-bcec53e6e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global average pooling\n",
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2636dbc3-2c66-43c3-bc42-08e33410e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.643388  , 0.59718215, 0.5825038 ],\n",
       "       [0.7630747 , 0.26010972, 0.10848834]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ff719-a704-4243-bf1a-64aa897d502e",
   "metadata": {},
   "source": [
    "## CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72237c1-121b-41df-8b6e-36430bfb8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads the mnist dataset, add the channels axis to the inputs,\n",
    "#              scales the values to the 0-1 range, and splits the dataset\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78204d-839e-4b56-9ea0-a4c526573554",
   "metadata": {},
   "source": [
    "### Basic CNN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d18178-2f3e-4861-bf66-d7575367caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/Desktop/env/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Used on Fashion MNIST\n",
    "from functools import partial\n",
    "\n",
    "DefaultConv2d = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                       activation='relu', kernel_initializer='he_normal')\n",
    "model = tf.keras.Sequential([\n",
    "    ### This first conv layer should capture lower level features so less filters\n",
    "    DefaultConv2d(filters=64, kernel_size=7, input_shape=[28,28,1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2d(filters=128),\n",
    "    DefaultConv2d(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    ### This conv layers should capture the high level features so have more filters\n",
    "    DefaultConv2d(filters=256),\n",
    "    DefaultConv2d(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b359727-f2be-40f5-815b-a7beaab0dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736732930.538899    4011 service.cc:148] XLA service 0x721860012720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736732930.539032    4011 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-01-12 19:48:50.606743: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  34/1719\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1678 - loss: 2.7955    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736732933.144613    4011 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6103 - loss: 1.0985 - val_accuracy: 0.8688 - val_loss: 0.3637\n",
      "Epoch 2/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8516 - loss: 0.4461 - val_accuracy: 0.8922 - val_loss: 0.3063\n",
      "Epoch 3/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8790 - loss: 0.3557 - val_accuracy: 0.8846 - val_loss: 0.3033\n",
      "Epoch 4/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8904 - loss: 0.3158 - val_accuracy: 0.8988 - val_loss: 0.2685\n",
      "Epoch 5/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9004 - loss: 0.2934 - val_accuracy: 0.9094 - val_loss: 0.2524\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "             optimizer='nadam',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688effd-387a-45ca-b93c-4cb37980aeda",
   "metadata": {},
   "source": [
    "### List of other Architectures\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- GoogLeNet\n",
    "- VGGNet\n",
    "- ResNet\n",
    "- Xception(A variant of GoogLeNet)\n",
    "- SENet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "- MobileNet\n",
    "- CSPNet\n",
    "- EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06cb8-10ce-4432-84c9-26411aee49ac",
   "metadata": {},
   "source": [
    "## Implementing a ResNet-34 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f23db35-7c40-4b7b-ac23-5019d4e05e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                       padding='same', kernel_initializer='he_normal',\n",
    "                       use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs \n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2875f4d-fe47-4d5e-852a-0df7a00e9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_34 = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    res_net_34.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "    \n",
    "res_net_34.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "res_net_34.add(tf.keras.layers.Flatten())\n",
    "res_net_34.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfe862-ae42-4c8c-b7e2-11ba24e2e888",
   "metadata": {},
   "source": [
    "## Using Pretrained Models from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d08d6ab-a0af-4149-8b78-ae1883b66b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a074114b-4daf-4337-9cd1-0b4bea56d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()['images']\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "667b3b65-d128-4395-a5a4-7380c3f6a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24de1240-6050-4468-bdd4-1a05fad41cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(inputs)\n",
    "y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88552efa-6b98-4a55-b565-5b5b8dea8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n03598930 - jigsaw_puzzle 30.61%\n",
      "  n02782093 - balloon      17.16%\n",
      "  n03888257 - parachute    5.58%\n",
      "  n06359193 - web_site     3.84%\n",
      "Image #1\n",
      "  n04209133 - shower_cap   34.36%\n",
      "  n09229709 - bubble       11.40%\n",
      "  n02782093 - balloon      9.46%\n",
      "  n07745940 - strawberry   4.94%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(y_proba, top=4)\n",
    "for image_index in range(len(images)):\n",
    "    print(f'Image #{image_index}')\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f'  {class_id} - {name:12s} {y_proba:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03639f-7258-4904-bfae-8b485ba58087",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71392c-cef0-4788-8fcd-4d8d98032c09",
   "metadata": {},
   "source": [
    "If you want to build an image classifier but don't have enough data to train it from scratch, it is often a good idea to reuse the lower layers of a pretrained model. <br>\n",
    "For this example we will reuse parts of the Xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cef7cc60-2532-494f-b739-fbf198f39e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset,info = tfds.load('tf_flowers', as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits['train'].num_examples\n",
    "class_names = info.features['label'].names\n",
    "n_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f61c0a-58ae-4ecf-8c32-87b3371e600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8c491a8-567b-49db-bc5c-6266b16e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before you can batch, you need to make sure all the images have same dimensions\n",
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "183f909f-d286-434d-b592-4d51733be714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70afdce-dc96-47c8-b6b4-117351311a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step \n"
     ]
    }
   ],
   "source": [
    "## Load pretrained model without the top layer\n",
    "base_model = tf.keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                    include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation='softmax')(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3332fd-731a-4120-a773-793a5dcd0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze the Xception model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa4164f7-7dc3-4856-b928-c23f2ac49fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 20:00:20.071205: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-01-12 20:00:21.418618: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-12 20:00:21.486262: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2025-01-12 20:00:21.521583: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 2464 bytes spill stores, 2460 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7082 - loss: 1.0316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 20:00:36.184982: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1030', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 137ms/step - accuracy: 0.7093 - loss: 1.0295 - val_accuracy: 0.8330 - val_loss: 0.6542\n",
      "Epoch 2/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.9146 - loss: 0.3402 - val_accuracy: 0.8603 - val_loss: 0.6304\n",
      "Epoch 3/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.9258 - loss: 0.2423 - val_accuracy: 0.8657 - val_loss: 0.5935\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccb04013-c15a-4523-bc92-e62acb119041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets unfreeze some of the layers and train again\n",
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96a0a879-f47d-403c-b8e6-f87ebd3819a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 176ms/step - accuracy: 0.7958 - loss: 0.7019 - val_accuracy: 0.5172 - val_loss: 5.8457\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - accuracy: 0.9143 - loss: 0.2761 - val_accuracy: 0.7350 - val_loss: 1.5171\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.9608 - loss: 0.1260 - val_accuracy: 0.8730 - val_loss: 0.4024\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.9694 - loss: 0.0894 - val_accuracy: 0.8693 - val_loss: 0.5044\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 144ms/step - accuracy: 0.9739 - loss: 0.0785 - val_accuracy: 0.9038 - val_loss: 0.2958\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 144ms/step - accuracy: 0.9889 - loss: 0.0353 - val_accuracy: 0.9093 - val_loss: 0.3785\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.9908 - loss: 0.0243 - val_accuracy: 0.8784 - val_loss: 0.6225\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.9863 - loss: 0.0607 - val_accuracy: 0.9002 - val_loss: 0.3672\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 147ms/step - accuracy: 0.9822 - loss: 0.0530 - val_accuracy: 0.9093 - val_loss: 0.3837\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 145ms/step - accuracy: 0.9970 - loss: 0.0119 - val_accuracy: 0.9201 - val_loss: 0.3676\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b3747-deea-49a2-b9ab-c1db4948cc0f",
   "metadata": {},
   "source": [
    "## Classification and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757d247-9ffd-4211-a99e-0bdb985ae667",
   "metadata": {},
   "source": [
    "Localization can be expressed as a regression task, to predict a bounding box around the object's center, predict the X cord, y cord, width and height of the box. Adding this part to the model is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97f7889b-c538-462d-8401-5a2e8c662eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input,\n",
    "                       outputs=[class_output, loc_output])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  \n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2],  # depends on what you care most about\n",
    "              optimizer=optimizer, metrics=[\"accuracy\", \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cbb15-e112-42d4-b0ae-40c75c6aaef6",
   "metadata": {},
   "source": [
    "<b>The main problem is we don't have these boxes in our original dataset. You would most likly have to do this yourself or use crowdsourcing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26680c8f-958e-436b-b9e2-e58cbae32bf8",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f38cb4-4696-43e3-9e53-271b4078819c",
   "metadata": {},
   "source": [
    "The task of classifying and localizing multiple objects in an image is called <b>Object Detection</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51692b-6a9a-40aa-bbbc-58258d34315e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
