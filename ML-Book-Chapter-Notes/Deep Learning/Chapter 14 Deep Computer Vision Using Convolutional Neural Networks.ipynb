{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3856214-dc1a-4e96-bf75-4e25cecc58e9",
   "metadata": {},
   "source": [
    "# Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b81b1-e249-4bee-beb2-3cc229e831cd",
   "metadata": {},
   "source": [
    "## Implementing Convolutional Layers with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72a4b78-6f08-4a09-8610-fe1f6bf9847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:54:29.688266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-16 15:54:29.822988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737064469.884861    7581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737064469.901012    7581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-16 15:54:30.065507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1737064472.030388    7581 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9712 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "## Get some sample images\n",
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "images = load_sample_images()['images']\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f4a54e-425d-4621-a733-cf7ff3c0176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape\n",
    "## 4d tensor\n",
    "## 2 Images in tensor\n",
    "## 70 is the height in pixels\n",
    "## 120 is the width in pixels\n",
    "## 3 is the amount of color channels(most likly RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448b0578-160e-4003-bf1f-2b3934367cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737064472.701948    7581 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc65c7cd-6e0d-4a0c-b339-650c99bd4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape\n",
    "## 32 channels (because of the filters)\n",
    "## The height and width shrunk because Conv2d doesn't zero pad by defualt\n",
    "## 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82430eb9-4acf-42ff-a956-aef66856198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
    "                                   padding='same') ## Zero padding\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc668b6e-2582-4c66-8517-2f29ed39d095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf9030a-8405-4471-9806-514e0a92ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "biases.shape\n",
    "## 1d shape [output_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16871da-c0a6-4c30-a64a-21a15a350493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels.shape\n",
    "## 4d shape [kernel_height, kernel_width, input_channels, output_channels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61920a4-bd8b-43f3-a26d-cefffa6f92a4",
   "metadata": {},
   "source": [
    "<b>You should also add activation functions and kernel_initilizers to Convolutional layers, for the same reasons as Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f7a70-fdf6-4acd-a7a4-152c504a70f5",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981accab-297f-4d9e-b18a-a2c6ad3ab340",
   "metadata": {},
   "source": [
    "Pooling layers are ment to subsample(i.e. shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters(thereby limiting the risk of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091ec1b-d983-48c9-a36b-fb696d335089",
   "metadata": {},
   "source": [
    "### Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b08449c-b7d5-4e0d-a563-34dff7557514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Pool\n",
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)\n",
    "## AveragePool\n",
    "avg_pool = tf.keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4159347-496b-40ba-bf31-8f669e83a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Depthwise pooling layer\n",
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1] // self.pool_size ## number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5038703-25cc-42c6-881d-bcec53e6e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global average pooling\n",
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2636dbc3-2c66-43c3-bc42-08e33410e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.643388  , 0.59718215, 0.5825038 ],\n",
       "       [0.7630747 , 0.26010972, 0.10848834]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ff719-a704-4243-bf1a-64aa897d502e",
   "metadata": {},
   "source": [
    "## CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72237c1-121b-41df-8b6e-36430bfb8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads the mnist dataset, add the channels axis to the inputs,\n",
    "#              scales the values to the 0-1 range, and splits the dataset\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78204d-839e-4b56-9ea0-a4c526573554",
   "metadata": {},
   "source": [
    "### Basic CNN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d18178-2f3e-4861-bf66-d7575367caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/Desktop/env/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Used on Fashion MNIST\n",
    "from functools import partial\n",
    "\n",
    "DefaultConv2d = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                       activation='relu', kernel_initializer='he_normal')\n",
    "model = tf.keras.Sequential([\n",
    "    ### This first conv layer should capture lower level features so less filters\n",
    "    DefaultConv2d(filters=64, kernel_size=7, input_shape=[28,28,1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2d(filters=128),\n",
    "    DefaultConv2d(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    ### This conv layers should capture the high level features so have more filters\n",
    "    DefaultConv2d(filters=256),\n",
    "    DefaultConv2d(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu', \n",
    "                          kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b359727-f2be-40f5-815b-a7beaab0dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737064475.000278    7649 service.cc:148] XLA service 0x7c77ec002640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737064475.000409    7649 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-01-16 15:54:35.068406: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  33/1719\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1368 - loss: 2.7736    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737064477.633120    7649 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.6034 - loss: 1.1166 - val_accuracy: 0.8682 - val_loss: 0.3799\n",
      "Epoch 2/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8372 - loss: 0.4731 - val_accuracy: 0.8892 - val_loss: 0.2994\n",
      "Epoch 3/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8749 - loss: 0.3702 - val_accuracy: 0.8948 - val_loss: 0.2983\n",
      "Epoch 4/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8901 - loss: 0.3214 - val_accuracy: 0.9058 - val_loss: 0.2638\n",
      "Epoch 5/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9015 - loss: 0.2910 - val_accuracy: 0.9064 - val_loss: 0.2649\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "             optimizer='nadam',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688effd-387a-45ca-b93c-4cb37980aeda",
   "metadata": {},
   "source": [
    "### List of other Architectures\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- GoogLeNet\n",
    "- VGGNet\n",
    "- ResNet\n",
    "- Xception(A variant of GoogLeNet)\n",
    "- SENet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "- MobileNet\n",
    "- CSPNet\n",
    "- EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06cb8-10ce-4432-84c9-26411aee49ac",
   "metadata": {},
   "source": [
    "## Implementing a ResNet-34 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f23db35-7c40-4b7b-ac23-5019d4e05e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                       padding='same', kernel_initializer='he_normal',\n",
    "                       use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs \n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2875f4d-fe47-4d5e-852a-0df7a00e9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_34 = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    res_net_34.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "    \n",
    "res_net_34.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "res_net_34.add(tf.keras.layers.Flatten())\n",
    "res_net_34.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfe862-ae42-4c8c-b7e2-11ba24e2e888",
   "metadata": {},
   "source": [
    "## Using Pretrained Models from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d08d6ab-a0af-4149-8b78-ae1883b66b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a074114b-4daf-4337-9cd1-0b4bea56d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()['images']\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "667b3b65-d128-4395-a5a4-7380c3f6a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24de1240-6050-4468-bdd4-1a05fad41cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(inputs)\n",
    "y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88552efa-6b98-4a55-b565-5b5b8dea8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n03598930 - jigsaw_puzzle 30.61%\n",
      "  n02782093 - balloon      17.17%\n",
      "  n03888257 - parachute    5.58%\n",
      "  n06359193 - web_site     3.83%\n",
      "Image #1\n",
      "  n04209133 - shower_cap   34.29%\n",
      "  n09229709 - bubble       11.40%\n",
      "  n02782093 - balloon      9.46%\n",
      "  n07745940 - strawberry   4.94%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(y_proba, top=4)\n",
    "for image_index in range(len(images)):\n",
    "    print(f'Image #{image_index}')\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f'  {class_id} - {name:12s} {y_proba:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03639f-7258-4904-bfae-8b485ba58087",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71392c-cef0-4788-8fcd-4d8d98032c09",
   "metadata": {},
   "source": [
    "If you want to build an image classifier but don't have enough data to train it from scratch, it is often a good idea to reuse the lower layers of a pretrained model. <br>\n",
    "For this example we will reuse parts of the Xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef7cc60-2532-494f-b739-fbf198f39e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset,info = tfds.load('tf_flowers', as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits['train'].num_examples\n",
    "class_names = info.features['label'].names\n",
    "n_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f61c0a-58ae-4ecf-8c32-87b3371e600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8c491a8-567b-49db-bc5c-6266b16e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before you can batch, you need to make sure all the images have same dimensions\n",
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "183f909f-d286-434d-b592-4d51733be714",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70afdce-dc96-47c8-b6b4-117351311a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pretrained model without the top layer\n",
    "base_model = tf.keras.applications.xception.Xception(weights='imagenet',\n",
    "                                                    include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation='softmax')(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed3332fd-731a-4120-a773-793a5dcd0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze the Xception model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa4164f7-7dc3-4856-b928-c23f2ac49fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:55:36.000396: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-01-16 15:55:37.401838: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-16 15:55:37.508244: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 2464 bytes spill stores, 2460 bytes spill loads\n",
      "\n",
      "2025-01-16 15:55:37.560790: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3894', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7053 - loss: 1.0292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:55:51.935858: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1030', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 134ms/step - accuracy: 0.7065 - loss: 1.0271 - val_accuracy: 0.8439 - val_loss: 0.6201\n",
      "Epoch 2/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 79ms/step - accuracy: 0.9138 - loss: 0.3261 - val_accuracy: 0.8639 - val_loss: 0.6183\n",
      "Epoch 3/3\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step - accuracy: 0.9327 - loss: 0.2378 - val_accuracy: 0.8711 - val_loss: 0.5963\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccb04013-c15a-4523-bc92-e62acb119041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets unfreeze some of the layers and train again\n",
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a0a879-f47d-403c-b8e6-f87ebd3819a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - accuracy: 0.7840 - loss: 0.7310 - val_accuracy: 0.5626 - val_loss: 30.5297\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9072 - loss: 0.3003 - val_accuracy: 0.8385 - val_loss: 0.5264\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9678 - loss: 0.1032 - val_accuracy: 0.8711 - val_loss: 0.4895\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 139ms/step - accuracy: 0.9730 - loss: 0.0945 - val_accuracy: 0.9074 - val_loss: 0.4278\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9766 - loss: 0.0850 - val_accuracy: 0.8929 - val_loss: 0.3458\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9867 - loss: 0.0402 - val_accuracy: 0.9056 - val_loss: 0.3207\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9866 - loss: 0.0457 - val_accuracy: 0.9020 - val_loss: 0.3412\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9904 - loss: 0.0292 - val_accuracy: 0.8966 - val_loss: 0.4234\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9907 - loss: 0.0291 - val_accuracy: 0.8966 - val_loss: 0.4928\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 140ms/step - accuracy: 0.9951 - loss: 0.0177 - val_accuracy: 0.9038 - val_loss: 0.4392\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b3747-deea-49a2-b9ab-c1db4948cc0f",
   "metadata": {},
   "source": [
    "## Classification and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757d247-9ffd-4211-a99e-0bdb985ae667",
   "metadata": {},
   "source": [
    "Localization can be expressed as a regression task, to predict a bounding box around the object's center, predict the X cord, y cord, width and height of the box. Adding this part to the model is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97f7889b-c538-462d-8401-5a2e8c662eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = tf.keras.layers.Dense(4)(avg)\n",
    "model = tf.keras.Model(inputs=base_model.input,\n",
    "                       outputs=[class_output, loc_output])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  \n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2],  # depends on what you care most about\n",
    "              optimizer=optimizer, metrics=[\"accuracy\", \"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cbb15-e112-42d4-b0ae-40c75c6aaef6",
   "metadata": {},
   "source": [
    "<b>The main problem is we don't have these boxes in our original dataset. You would most likly have to do this yourself or use crowdsourcing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26680c8f-958e-436b-b9e2-e58cbae32bf8",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f38cb4-4696-43e3-9e53-271b4078819c",
   "metadata": {},
   "source": [
    "The task of classifying and localizing multiple objects in an image is called <b>Object Detection</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531ccf7-3f31-4276-b80e-8a2df5b65001",
   "metadata": {},
   "source": [
    "### Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e26b35-aa92-4398-be43-ae04ade91522",
   "metadata": {},
   "source": [
    "Fully Convolutional Networks(FCNs) are a way to slide a CNN across an image(to perform object detection) much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85513586-a57a-4cde-9947-256fa9ccc8b0",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bc3f6-2c9c-4deb-97a2-ff946268b57a",
   "metadata": {},
   "source": [
    "YOLO(You Only Look Once) is a fast and accurate object detection architecture. It is so fast it can even work on videos. The architecture is similar to FCNs but with key differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea813f-e3c9-4be3-8f0c-cdfa39f99a15",
   "metadata": {},
   "source": [
    "## Object Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdfc7c-1c79-4a42-86a3-4fea0d37cfc8",
   "metadata": {},
   "source": [
    "Object tracking is a challenging task because objects move, they may grow or shrink in size, their appearance may change as they turn around or move to different lighting, etc. One of the most popular object tracking systems is DeepSORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba63cae-fc25-428d-a0a9-cf0f0eb6118a",
   "metadata": {},
   "source": [
    "## Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd8b8e-5089-44f9-8823-3ae36e2ab543",
   "metadata": {},
   "source": [
    "In semantic segmentation each pixel is classified according to the class of the object it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a2d22-a735-4349-93c5-04b0b43fcc65",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28272e-b0bf-4c0a-8825-3dc87d76ae95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
