{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba9eafc-d08a-40b8-86c7-a2fe6212bc51",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03aca84-5356-4444-82c7-4777b28efee2",
   "metadata": {},
   "source": [
    "Dimensionality Reduction is used to reduce the amount of features in a dataset without losing valuable information/connections. It can also help with data visualizations and make the data easier to understand. Reducing Dimensionality does cause info loss. It may speed up training but will most likely hurt the performance of the model.<br> <b>Try training with full dataset before using any Dimensionality reduction</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73956f-dab0-48c5-b6b1-764d688c8a03",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality \n",
    "Because of the nature of dimensions, the higher the dimension, the more space you have. In high dimensional datasets, the distance between instances can be very large. <b>The more dimensions in a dataset, the more likely you will overfit.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdea419-8ce6-4b5c-bd38-1639e5e65819",
   "metadata": {},
   "source": [
    "## Main Approaches for D-Reduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6eb0a-e6c9-458c-bd83-d4568543efe4",
   "metadata": {},
   "source": [
    "- Projection\n",
    "- Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497b3b2-1ca2-4823-93a1-5533747542f0",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Principal component analysis is the most popular dimensionality reduction algorithm. The main idea of PCA is to minimize the mean squared distance between the original dataset and its projection onto the new axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81d7827-1e35-4b70-a75c-9587ef3367a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Making a 3d Dataset\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030ca6a3-51cb-477e-9989-9f8213d4eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # n_components is what dimension to make the dataset into(ex. 3d to 2d)\n",
    "X2d = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d9a45d-5339-49fe-af13-b33c10d40201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This ratio indicates the proportion of the dataset's varience that lies along each prinicpal component.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe93d71-609f-4538-af24-6c8ef1637eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
