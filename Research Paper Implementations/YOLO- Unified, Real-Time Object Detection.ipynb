{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "571a000e-c062-46af-a253-baf0bf1607b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import kaggle\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9847c-39e4-4c13-ad15-ff6927b622f4",
   "metadata": {},
   "source": [
    "# Implmentation Outline\n",
    "- Do pretraining for image classification to learn features\n",
    "- Convert model to object detection\n",
    "- Make custom loss function\n",
    "- Get object detection dataset\n",
    "- Train according to paper specifications\n",
    "- Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99868e8-e5f1-4043-96ac-f4fa10536841",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768032b-48d7-4db5-8355-34332ef4a9c5",
   "metadata": {},
   "source": [
    "For pretraining I will use a smaller version of the ImageNet dataset then the one used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75773c7-4775-443e-9521-86eb2ecf2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/misha/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "## Load pretraining data\n",
    "kaggle.api.authenticate()\n",
    "# kaggle.api.dataset_download_files('ifigotin/imagenetmini-1000',\n",
    "#                                 path='/home/misha/Desktop/data/yolo_paper/pretrain_data/image_data/',\n",
    "#                                 unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16382490-52ca-471c-a403-094bc945b88d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a11c76-1384-405f-a394-1e64c384e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_DATA_PATH = '/home/misha/Desktop/data/yolo_paper/pretrain_data/'\n",
    "labels_txt = PRETRAIN_DATA_PATH + 'words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7061ee60-036e-40e0-9699-9ba22203a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(labels_txt, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7c8a0a-f5ae-48ac-bb8a-cfe23b0639f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n00001740</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n00001930</td>\n",
       "      <td>physical entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n00002137</td>\n",
       "      <td>abstraction, abstract entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n00002452</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n00002684</td>\n",
       "      <td>object, physical object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82110</th>\n",
       "      <td>n15299225</td>\n",
       "      <td>study hall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82111</th>\n",
       "      <td>n15299367</td>\n",
       "      <td>Transfiguration, Transfiguration Day, August 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82112</th>\n",
       "      <td>n15299585</td>\n",
       "      <td>usance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82113</th>\n",
       "      <td>n15299783</td>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82114</th>\n",
       "      <td>n15300051</td>\n",
       "      <td>9/11, 9-11, September 11, Sept. 11, Sep 11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82115 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                          object\n",
       "0      n00001740                                          entity\n",
       "1      n00001930                                 physical entity\n",
       "2      n00002137                    abstraction, abstract entity\n",
       "3      n00002452                                           thing\n",
       "4      n00002684                         object, physical object\n",
       "...          ...                                             ...\n",
       "82110  n15299225                                      study hall\n",
       "82111  n15299367  Transfiguration, Transfiguration Day, August 6\n",
       "82112  n15299585                                          usance\n",
       "82113  n15299783                                          window\n",
       "82114  n15300051      9/11, 9-11, September 11, Sept. 11, Sep 11\n",
       "\n",
       "[82115 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf99d89a-80fd-45ff-ab97-e224fdd06086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[labels['code'] == 'n03485794214']['object'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c5f4fa-64a1-49f0-9d40-bef2f29e9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce9eea8-3503-48bf-9abe-9cc8275995ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788b2458-8146-4355-8e44-c324c48ee516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef15855-1ce8-4db8-91fb-862de7c69c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/') == os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b3fa5a-cfc8-441c-9426-6e4b8329dded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'train']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98047eee-3243-493b-9ff5-6188212f23da",
   "metadata": {},
   "source": [
    "#### Rename the folders to their label instead of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93198f35-4b40-478e-9a04-d11fdc514e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for split in os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/'):\n",
    "#     for folder in os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/' + split):\n",
    "#         try:\n",
    "#             label = labels[labels['code'] == folder]['object'].values[0]\n",
    "#         except IndexError as err:\n",
    "#             print(f\"No label found for Split: {split} Folder: {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         ## Rename folder\n",
    "#         source_path = PRETRAIN_DATA_PATH + f'image_data/imagenet-mini/{split}/{folder}'\n",
    "#         destination_path = PRETRAIN_DATA_PATH + f'image_data/imagenet-mini/{split}/{label}'\n",
    "\n",
    "#         try:\n",
    "#             shutil.move(source_path, destination_path)\n",
    "#         except OSError as err:\n",
    "#             print(f\"Error moving folder({folder}): {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1365e4f4-5e7d-4fb4-8901-6860b67b2ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/') == os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceefe39-b5f7-4a70-bf6c-0d88ca5b8621",
   "metadata": {},
   "source": [
    "#### Move load data into Tensorflow datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7248ac7-3587-47bc-990f-ec494d716874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = pathlib.Path(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')\n",
    "val_dir = pathlib.Path(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f97530c-804f-43e5-926b-afd909fb5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34745 files belonging to 999 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737821254.121944    4468 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9617 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3923 files belonging to 999 classes.\n"
     ]
    }
   ],
   "source": [
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir, \n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    batch_size=32, \n",
    "    label_mode='categorical',   ## Vector Representation (Use categorical_crossentropy loss)\n",
    "    image_size=(224, 224),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    seed=1, \n",
    "    shuffle=True\n",
    ")\n",
    "val_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir, \n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    batch_size=32, \n",
    "    label_mode='categorical',   ## Vector Representation (Use categorical_crossentropy loss)\n",
    "    image_size=(224, 224),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    seed=1, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1562c-060d-43c9-905c-213607623763",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dc5ad9-b913-49ea-81e1-bdc1c26cb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_augmentation = tf.keras.Sequential([\n",
    "#     tf.keras.layers.RandomFlip('horizontal', seed=1),\n",
    "#     tf.keras.layers.RandomRotation(0.2, seed=1)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98644bb-e013-45bd-8e90-4fb2a1aad202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aug = train_df.map(lambda x, y: (data_augmentation(x), y))\n",
    "# val_aug = val_df.map(lambda x, y: (data_augmentation(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e88c1c-1671-4c1a-a6c2-4f4730dc7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.concatenate(train_aug).prefetch(1)\n",
    "# val_df = val_df.concatenate(val_aug).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87410a31-e9e1-457a-8cdd-1fd2082b4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_df) * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf47c14a-8aad-4e40-a67e-4c4e805ea34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val_df) * 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc8f25-1a8c-422a-8d5d-fa86dfba98da",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b5d730d-2b96-40f3-bcae-ba062233bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial layers\n",
    "# Conv2DLayer = partial(tf.keras.layers.Conv2D, strides=(1,1), padding='same',\n",
    "#                       kernel_initializer='he_normal')\n",
    "# MaxPoolLayer = partial(tf.keras.layers.MaxPool2D, pool_size=(2,2), strides=2, padding='same',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b20a88f-f782-4b68-aec1-7746e8ad3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_model = tf.keras.Sequential()\n",
    "\n",
    "# # Input layer (only for pretraining, will be removed when applied to full model)\n",
    "# pretraining_model.add(tf.keras.layers.InputLayer(shape=(224,224,3)))\n",
    "\n",
    "# ## First 20 layers according to paper\n",
    "# pretraining_model.add(Conv2DLayer(filters=64, strides=(2,2), kernel_size=(7,7)))\n",
    "# pretraining_model.add(MaxPoolLayer())\n",
    "# pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# pretraining_model.add(Conv2DLayer(filters=192, kernel_size=(3,3)))\n",
    "# pretraining_model.add(MaxPoolLayer())\n",
    "# pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# pretraining_model.add(Conv2DLayer(filters=128, kernel_size=(1,1)))\n",
    "# pretraining_model.add(Conv2DLayer(filters=256, kernel_size=(3,3)))\n",
    "# pretraining_model.add(Conv2DLayer(filters=256, kernel_size=(1,1)))\n",
    "# pretraining_model.add(Conv2DLayer(filters=512, kernel_size=(3,3)))\n",
    "# pretraining_model.add(MaxPoolLayer())\n",
    "# pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# for _ in range(4):\n",
    "#     pretraining_model.add(Conv2DLayer(filters=256, kernel_size=(1,1)))\n",
    "#     pretraining_model.add(Conv2DLayer(filters=512, kernel_size=(3,3)))\n",
    "#     pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "# pretraining_model.add(Conv2DLayer(filters=512, kernel_size=(1,1)))\n",
    "# pretraining_model.add(Conv2DLayer(filters=1024, kernel_size=(3,3)))\n",
    "# pretraining_model.add(MaxPoolLayer())\n",
    "# pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# ## Flatten Layer\n",
    "# pretraining_model.add(tf.keras.layers.Flatten())\n",
    "# pretraining_model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# ## Output layer for pretraining (will be removed when layers are reused)\n",
    "# pretraining_model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f85675-0924-4f1b-b0be-1e5c6de1e5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretraining_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c76f2e9-ba2b-4d37-ac21-8224a4b8fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loaded model from checkpoint\n",
    "# pretraining_model = tf.keras.models.load_model('best_pretrain_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a357d5a-0f60-4783-bdb6-ab3fe1b782e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretraining\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9, nesterov=True)\n",
    "# pretraining_model.compile(optimizer=optimizer,\n",
    "#                           loss=tf.keras.losses.categorical_crossentropy,\n",
    "#                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a6796e2-a6af-4ef1-b1b0-59bf5ba73e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = tf.keras.callbacks.ModelCheckpoint('best_pretrain_model.keras', monitor='val_accuracy',\n",
    "#                                                  verbose=1, save_best_only=True)\n",
    "# tb_dir = 'logs/yolo/pretrain/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir)\n",
    "\n",
    "# pretraining_model.fit(train_df, validation_data=val_df, epochs=25, \n",
    "#                      callbacks=[checkpoints, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40e349-b32f-46e1-b7b9-74233a81819f",
   "metadata": {},
   "source": [
    "## Using transfer learning for pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5183a4-ea08-4806-9652-a308adfd2821",
   "metadata": {},
   "source": [
    "In order to save time I will just do transfer learning for this pretraining since it shouldn't affect the actual model a lot.\n",
    "\n",
    "We will use EfficientNetV2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24a6ff-d0ef-464d-a121-fed5cebf1aef",
   "metadata": {},
   "source": [
    "### Model fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e188bcf-2488-40b5-a1df-8cd63e73db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_func = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "train_df = train_df.map(lambda X, y: (preprocess_func(X), y)).prefetch(2)\n",
    "val_df = val_df.map(lambda X, y: (preprocess_func(X), y)).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9ae3563-607e-478d-a7d7-93f10f24cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pretrain_model = tf.keras.applications.efficientnet_v2.EfficientNetV2M(weights='imagenet', \n",
    "                                                                           include_top=False,\n",
    "                                                                           input_shape=(224, 224, 3))\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_pretrain_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation='softmax')(avg)\n",
    "model = tf.keras.Model(inputs=base_pretrain_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a962461-734d-4fd4-b5be-6449987ecded",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_pretrain_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "482c847c-fc1b-48ed-abab-967580c41bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# history = model.fit(train_df, validation_data=val_df, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b593653-4978-4ba8-83f6-43fb3b110e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "410b46fb-422f-4937-a0b2-9cd503427066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_pretrain_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2adf5d3-0a7a-44e3-8f81-58fbfd66ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unfreeze some layers and retrain\n",
    "for layer in base_pretrain_model.layers[:75]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2711393d-174b-4548-8391-3fbbec5cd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-5)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights=True, patience=2)\n",
    "\n",
    "# history = model.fit(train_df, validation_data=val_df, epochs=10, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ef5793c-dc52-4ab3-b5da-24d340d543eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('pretrain_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b73db34-8a8e-42ed-a274-997780716a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pretrain_model.keras', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fa04f-257f-4085-9f11-e31dd21383bc",
   "metadata": {},
   "source": [
    "## YOLO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c3c05-2ca9-4955-b245-5f9fb5dc01ca",
   "metadata": {},
   "source": [
    "### Outline for building\n",
    "- Get Dataset Pascal VOC or whatever is said in paper\n",
    "- Build loss function\n",
    "- Figure out infrence\n",
    "- Build Model class\n",
    "- Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b051f65-b418-4732-a790-dd05e25e7936",
   "metadata": {},
   "source": [
    "### Download and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b834a65-c14b-4f6b-bb8d-0654bfd18846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset\n"
     ]
    }
   ],
   "source": [
    "kaggle.api.dataset_download_files('gopalbhattrai/pascal-voc-2012-dataset',\n",
    "                                 path='/home/misha/Desktop/data/yolo_paper/train_data',\n",
    "                                 unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04bddeff-e96f-4dab-b360-51787b78ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATAPATH = '/home/misha/Desktop/data/yolo_paper/train_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f97ca43e-aa99-4c12-9a8f-ae5faf2c5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_coords(bbox):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    center_x = (xmin + xmax) / 2\n",
    "    center_y = (ymin + ymax) / 2\n",
    "    return center_x, center_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693145c4-d183-4be7-9468-601dca0d4647",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1920750134.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    images = []\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# def load_pascal_voc_data(image_dir, annotation_dir, image_set_file):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "\n",
    "    with open(image_set_file, 'r') as f:\n",
    "        ## Get list of image ids for the split\n",
    "        image_ids = [line.strip() for line in f]\n",
    "\n",
    "    for image_id in image_ids:\n",
    "        ## For each id, get corresponding image and annotation file\n",
    "        image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "        annotation_path = os.path.join(annotation_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        ## Load the image\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path)\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "        ## Parse the XML file\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name')\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(bbox.find('xmin').text)\n",
    "            ymin = int(bbox.find('ymin').text)\n",
    "            xmax = int(bbox.find('xmax').text)\n",
    "            ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "            ## Get actual features\n",
    "            center_x, center_y = get_center_coords([xmin, ymin, xmax, ymax])\n",
    "            # Calculate width and height\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "\n",
    "            bboxes.append([center_x, center_y, width, height])\n",
    "            labels.append(label)\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "    return images, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f5b58-5dcc-413d-bb76-c4cba22ef839",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '/home/misha/Desktop/data/yolo_paper/train_data/VOC2012_train_val/VOC2012_train_val/JPEGImages'\n",
    "annotation_dir = '/home/misha/Desktop/data/yolo_paper/train_data/VOC2012_train_val/VOC2012_train_val/Annotations'\n",
    "image_set_file = '/home/misha/Desktop/data/yolo_paper/train_data/VOC2012_train_val/VOC2012_train_val/ImageSets/Main/train.txt'\n",
    "\n",
    "images, bboxes, labels = load_pascal_voc_data(image_dir, annotation_dir, image_set_file)\n",
    "\n",
    "train_df = tf.data.Dataset.from_tensor_slices((images, bboxes, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc7776-a3a5-431a-bd45-eda82aae32ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3f909-94ce-44ca-b77e-030a9541bd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e55f52eb-cc8b-4fd2-a450-e252d20d5389",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928d2541-3092-4aaf-abf6-6f6e5871e507",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (394256117.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class YOLOLoss(tf.keras.losses.Loss):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917343e-bc19-4c67-b3b6-ddb875bb2aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
