{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "571a000e-c062-46af-a253-baf0bf1607b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import kaggle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9847c-39e4-4c13-ad15-ff6927b622f4",
   "metadata": {},
   "source": [
    "# Implmentation Outline\n",
    "- Do pretraining for image classification to learn features\n",
    "- Convert model to object detection\n",
    "- Make custom loss function\n",
    "- Get object detection dataset\n",
    "- Train according to paper specifications\n",
    "- Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99868e8-e5f1-4043-96ac-f4fa10536841",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768032b-48d7-4db5-8355-34332ef4a9c5",
   "metadata": {},
   "source": [
    "For pretraining I will use a smaller version of the ImageNet dataset then the one used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75773c7-4775-443e-9521-86eb2ecf2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/misha/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "## Load pretraining data\n",
    "kaggle.api.authenticate()\n",
    "# kaggle.api.dataset_download_files('ifigotin/imagenetmini-1000',\n",
    "#                                 path='/home/misha/Desktop/data/yolo_paper/pretrain_data/image_data/',\n",
    "#                                 unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16382490-52ca-471c-a403-094bc945b88d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a11c76-1384-405f-a394-1e64c384e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_DATA_PATH = '/home/misha/Desktop/data/yolo_paper/pretrain_data/'\n",
    "labels_txt = PRETRAIN_DATA_PATH + 'words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7061ee60-036e-40e0-9699-9ba22203a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(labels_txt, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7c8a0a-f5ae-48ac-bb8a-cfe23b0639f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n00001740</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n00001930</td>\n",
       "      <td>physical entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n00002137</td>\n",
       "      <td>abstraction, abstract entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n00002452</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n00002684</td>\n",
       "      <td>object, physical object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82110</th>\n",
       "      <td>n15299225</td>\n",
       "      <td>study hall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82111</th>\n",
       "      <td>n15299367</td>\n",
       "      <td>Transfiguration, Transfiguration Day, August 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82112</th>\n",
       "      <td>n15299585</td>\n",
       "      <td>usance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82113</th>\n",
       "      <td>n15299783</td>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82114</th>\n",
       "      <td>n15300051</td>\n",
       "      <td>9/11, 9-11, September 11, Sept. 11, Sep 11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82115 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                          object\n",
       "0      n00001740                                          entity\n",
       "1      n00001930                                 physical entity\n",
       "2      n00002137                    abstraction, abstract entity\n",
       "3      n00002452                                           thing\n",
       "4      n00002684                         object, physical object\n",
       "...          ...                                             ...\n",
       "82110  n15299225                                      study hall\n",
       "82111  n15299367  Transfiguration, Transfiguration Day, August 6\n",
       "82112  n15299585                                          usance\n",
       "82113  n15299783                                          window\n",
       "82114  n15300051      9/11, 9-11, September 11, Sept. 11, Sep 11\n",
       "\n",
       "[82115 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf99d89a-80fd-45ff-ab97-e224fdd06086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[labels['code'] == 'n03485794214']['object'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c5f4fa-64a1-49f0-9d40-bef2f29e9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce9eea8-3503-48bf-9abe-9cc8275995ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788b2458-8146-4355-8e44-c324c48ee516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef15855-1ce8-4db8-91fb-862de7c69c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/') == os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b3fa5a-cfc8-441c-9426-6e4b8329dded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'train']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98047eee-3243-493b-9ff5-6188212f23da",
   "metadata": {},
   "source": [
    "#### Rename the folders to their label instead of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93198f35-4b40-478e-9a04-d11fdc514e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for split in os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/'):\n",
    "#     for folder in os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/' + split):\n",
    "#         try:\n",
    "#             label = labels[labels['code'] == folder]['object'].values[0]\n",
    "#         except IndexError as err:\n",
    "#             print(f\"No label found for Split: {split} Folder: {folder}\")\n",
    "#             continue\n",
    "\n",
    "#         ## Rename folder\n",
    "#         source_path = PRETRAIN_DATA_PATH + f'image_data/imagenet-mini/{split}/{folder}'\n",
    "#         destination_path = PRETRAIN_DATA_PATH + f'image_data/imagenet-mini/{split}/{label}'\n",
    "\n",
    "#         try:\n",
    "#             shutil.move(source_path, destination_path)\n",
    "#         except OSError as err:\n",
    "#             print(f\"Error moving folder({folder}): {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1365e4f4-5e7d-4fb4-8901-6860b67b2ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/') == os.listdir(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceefe39-b5f7-4a70-bf6c-0d88ca5b8621",
   "metadata": {},
   "source": [
    "#### Move load data into Tensorflow datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7248ac7-3587-47bc-990f-ec494d716874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = pathlib.Path(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/train/')\n",
    "val_dir = pathlib.Path(PRETRAIN_DATA_PATH + 'image_data/imagenet-mini/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f97530c-804f-43e5-926b-afd909fb5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34745 files belonging to 999 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737638447.138351    4124 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9716 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3923 files belonging to 999 classes.\n"
     ]
    }
   ],
   "source": [
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir, \n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    batch_size=32, \n",
    "    label_mode='categorical',   ## Vector Representation (Use categorical_crossentropy loss)\n",
    "    image_size=(224, 224),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    seed=1, \n",
    "    shuffle=True\n",
    ")\n",
    "val_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir, \n",
    "    labels='inferred',\n",
    "    color_mode='rgb',\n",
    "    batch_size=32, \n",
    "    label_mode='categorical',   ## Vector Representation (Use categorical_crossentropy loss)\n",
    "    image_size=(224, 224),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    seed=1, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1562c-060d-43c9-905c-213607623763",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7dc5ad9-b913-49ea-81e1-bdc1c26cb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal', seed=1),\n",
    "    tf.keras.layers.RandomRotation(0.2, seed=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98644bb-e013-45bd-8e90-4fb2a1aad202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = train_df.map(lambda x, y: (data_augmentation(x), y))\n",
    "val_aug = val_df.map(lambda x, y: (data_augmentation(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5e88c1c-1671-4c1a-a6c2-4f4730dc7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.concatenate(train_aug).prefetch(1)\n",
    "val_df = val_df.concatenate(val_aug).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87410a31-e9e1-457a-8cdd-1fd2082b4a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69504"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df) * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf47c14a-8aad-4e40-a67e-4c4e805ea34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7872"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df) * 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc8f25-1a8c-422a-8d5d-fa86dfba98da",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197e78d2-7174-4844-96b9-641b85954eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FullConvLayer(tf.keras.layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Custom layer to hold the full convolution block. This is done to reduce copy and paste, and to make code simple.\n",
    "#     Always follows this simple structure:\n",
    "    \n",
    "#     Convolutional layers\n",
    "#     Maxpool layer\n",
    "#     \"\"\"\n",
    "#     def __init__(self, conv_map:dict, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Takes a dictionary/map where each key represents one convolution layer, and the value pair is a map \n",
    "#         with parameter values (i.e. {'filters': 10, 'kernel_size': (2,3), etc...})\n",
    "#         \"\"\"\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.layers = []\n",
    "#         for layer, param_map in list(conv_map.items()):\n",
    "#             self.layers.append(tf.keras.layers.Conv2D(**param_map))\n",
    "\n",
    "#         ## Append maxpool layer to block\n",
    "#         self.layers.append(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "                           \n",
    "#     def call(self, inputs):\n",
    "#         for i in range(self.layers):\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b24c70-e893-4fc1-8d47-f7aa81672e19",
   "metadata": {},
   "source": [
    "# Focus on general architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d0695dd-b02f-4c04-81fe-498854c22785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainingModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    This is the model that will be used for the pretrained layers in the YOLO network\n",
    "    Will have a export method that will export the layers needed for the full YOLO network\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_layer = tf.keras.layers.Input(shape=(224, 224, 3), name='input_layer') ## Standard according to paper\n",
    "\n",
    "        ## First 20 convolution layers of the network\n",
    "        ## All layers done according to paper standards\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20a88f-f782-4b68-aec1-7746e8ad3368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
